{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "import copy\n",
    "import keras\n",
    "from collections import Counter\n",
    "from node2vec import Node2Vec\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from stellargraph.layer import link_classification, GraphSAGE, GCN\n",
    "from stellargraph.data import BiasedRandomWalk, EdgeSplitter, UniformRandomWalk, UnsupervisedSampler\n",
    "from stellargraph.mapper import FullBatchNodeGenerator, Node2VecLinkGenerator, Node2VecNodeGenerator, GraphSAGENodeGenerator, GraphSAGELinkGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание - Предсказание уровня экспресси белка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.researchgate.net/publication/313504607/figure/fig3/AS:459880453677066@1486655453033/Protein-protein-interaction-PPI-network-of-DEGs-by-STRING-The-interaction-score-was.png'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Про биологию</b>\n",
    "    \n",
    "Экспрессия — процесс, в ходе которого наследственная информация от гена (последовательности нуклеотидов ДНК) преобразуется в функциональный продукт — белок. Уровнем экспрессии называют - количество белка, производящегося в этом процессе. Чем выше экспрессия белка, тем большее количество этого белка появляется в клетках человека. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">    \n",
    "<b>Важность задачи</b>\n",
    "    \n",
    "Существует множество причин необходимости в знании уровня экспресии белка. Например - это позволяет ученым разрабатывать лекарственные средства и оптимизировать их разработку. Теперь вам предстоит побыть в роли биоинформатика и помочь науке!\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Про Датасет</b>\n",
    "    \n",
    "Датасет представляет собой граф взаимойдествия белков. Где узлы это белки, взаимодействие между белками это ребро. \n",
    "\n",
    "Для каждого белка известен уровень его экспрессии. Ниже приведен список ребер `edges`. Информация по экспрессии белков, разбитая на `train` и `test`.\n",
    "   \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>344</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>344</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>344</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>344</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>344</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   node_1  node_2\n",
       "0     344      50\n",
       "1     344     153\n",
       "2     344     532\n",
       "3     344     679\n",
       "4     344     986"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Список ребер графа \n",
    "\n",
    "edges = pd.read_csv(\"https://raw.githubusercontent.com/a-milenkin/Otus_HW_protein_expression/main/edges.csv\", sep=\",\") # Подгрузим данные\n",
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.251968</td>\n",
       "      <td>11142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.689541</td>\n",
       "      <td>2243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.678245</td>\n",
       "      <td>15514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.272500</td>\n",
       "      <td>20944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.248888</td>\n",
       "      <td>8721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target   node\n",
       "0  0.251968  11142\n",
       "1  0.689541   2243\n",
       "2  0.678245  15514\n",
       "3  0.272500  20944\n",
       "4  0.248888   8721"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подгрузим тренирочную выборку\n",
    "train = pd.read_csv(\"https://raw.githubusercontent.com/a-milenkin/Otus_HW_protein_expression/main/train.csv\", sep=\",\") # Подгрузим данные\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.279231</td>\n",
       "      <td>817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.380795</td>\n",
       "      <td>9574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.686527</td>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.303594</td>\n",
       "      <td>4782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.367374</td>\n",
       "      <td>24125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target   node\n",
       "0  0.279231    817\n",
       "1  0.380795   9574\n",
       "2  0.686527   1607\n",
       "3  0.303594   4782\n",
       "4  0.367374  24125"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подгрузим отложенную выборку для валидации\n",
    "test = pd.read_csv(\"https://raw.githubusercontent.com/a-milenkin/Otus_HW_protein_expression/main/test.csv\", sep=\",\")\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Про Задачу</b>\n",
    "    \n",
    "Вам предлагается предсказать экспрессию белков (`target`) по приведенным данным для отложенной выборки. Ответы в отложенной выборке `test` даны вам для самостоятельной валидации.\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Замечание и комментарии</b>\n",
    "    \n",
    "    \n",
    "\n",
    "По ряду причин датасет был упрощен так, чтобы выполнялись следующие условия:\n",
    "* у графа одна компонента связанности. \n",
    "* удалены слишком крупные хабы\n",
    "* плотность связей графа уменьшена\n",
    "* решить задачу можно классическими ML подходами\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Оценка результатов</b>\n",
    "    \n",
    "\n",
    "\n",
    "Оценка точности модели будет оцениваться по метрике MSE на отложенной выборке `test`\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Автор задачи</b>\n",
    "\n",
    "По всем дополнительным вопросами писать Александру Миленькину\n",
    "* Телеграмм: Alerin75infskin\n",
    "* Почта: milenkin.aa@phystech.edu\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepairing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node: 10000, edges: 594174\n"
     ]
    }
   ],
   "source": [
    "edges_gnx = nx.from_pandas_edgelist(edges, \"node_1\", \"node_2\", create_using=nx.Graph())\n",
    "print(f\"node: {len(edges_gnx.nodes)}, edges: {len(edges_gnx.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = edges.copy()\n",
    "df['weight'] = 1\n",
    "df = df.pivot(index='node_1', columns='node_2', values='weight').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_stats = {}\n",
    "for node in df.index:\n",
    "    dict_stats[node] = df.loc[node].sum()  # number of edges for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of edges\n",
    "pd.Series(dict_stats.values()).describe()\n",
    "\n",
    "# print(f\"mean : {int(sum(dict_stats.values()) / len(dict_stats.values()))}\")\n",
    "# print(f\"min : {min(dict_stats.values())}\")\n",
    "# print(f\"max : {max(dict_stats.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw_networkx(edges_gnx)  # it doesn't make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.49 ms, sys: 0 ns, total: 7.49 ms\n",
      "Wall time: 7.42 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "degree_centrality = nx.degree_centrality(edges_gnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 37s, sys: 111 ms, total: 13min 37s\n",
      "Wall time: 13min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "closeness_centrality = nx.closeness_centrality(edges_gnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54min 17s, sys: 380 ms, total: 54min 18s\n",
      "Wall time: 54min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "betweenness_centrality = nx.betweenness_centrality(edges_gnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_lr = pd.read_csv('centralities_data.csv').rename(columns={'Unnamed: 0': 'node'}).set_index('node')\n",
    "\n",
    "degree_centrality = data_for_lr['degree_centrality']\n",
    "closeness_centrality = data_for_lr['closeness_centrality']\n",
    "betweenness_centrality = data_for_lr['betweenness_centrality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr only with centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_lr = pd.concat([\n",
    "    pd.DataFrame(pd.Series(degree_centrality), columns=['degree_centrality']),\n",
    "    pd.DataFrame(pd.Series(closeness_centrality), columns=['closeness_centrality']),\n",
    "    pd.DataFrame(pd.Series(betweenness_centrality), columns=['betweenness_centrality'])],\n",
    "    axis=1)\n",
    "\n",
    "Train = data_for_lr.merge(train.set_index('node'), how='inner', left_index=True, right_index=True)\n",
    "Test = data_for_lr.merge(test.set_index('node'), how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011354069147677646"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LinearRegression()\n",
    "\n",
    "lr_model.fit(Train[['degree_centrality', 'closeness_centrality', 'betweenness_centrality']], Train['target'])\n",
    "\n",
    "Test['score'] = lr_model.predict(Test[['degree_centrality', 'closeness_centrality', 'betweenness_centrality']])\n",
    "mean_squared_error(Test['target'], Test['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN for regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim) \n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim) \n",
    "        self.batchnorm2 = torch.nn.BatchNorm1d(hidden_dim) \n",
    "        self.act2 = torch.nn.ReLU()\n",
    "        \n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.act1(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.act2(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        # out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get embeddings with random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_sg = StellarGraph.from_networkx(edges_gnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 33s, sys: 1.18 s, total: 20min 34s\n",
      "Wall time: 20min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rw_instance = BiasedRandomWalk(edges_sg)\n",
    "\n",
    "walks = rw_instance.run(nodes=list(edges_sg.nodes()),\n",
    "                        length=50,\n",
    "                        n=20,\n",
    "                        p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffle walks\n",
    "random.shuffle(walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 244 ms, total: 1min 34s\n",
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vector_size = 40  # just because it's about median for edges by each node\n",
    "embeddings_model = Word2Vec(walks,\n",
    "                            vector_size=vector_size,\n",
    "                            workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8016675 , -3.0037794 , -2.7069683 ,  1.9127227 , -1.9025455 ,\n",
       "       -1.5921662 ,  0.41452155, -1.3604666 , -2.9584973 , -0.0200114 ,\n",
       "        1.3894318 ,  1.2748578 ,  5.2002425 , -3.41285   , -0.10872174,\n",
       "       -2.4098065 , -1.6019953 ,  3.2309704 , -0.30451548, -0.05676685,\n",
       "       -2.0679681 , -3.0840073 ,  0.5549966 ,  0.20392409,  1.0632231 ,\n",
       "        0.45110613,  1.908187  , -1.0702413 ,  0.5472529 ,  0.9929532 ,\n",
       "       -0.51793694,  1.5407206 , -1.2303212 ,  2.00657   ,  2.104186  ,\n",
       "       -1.0143015 , -0.4962665 ,  5.110513  , -1.3883709 , -1.474673  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_model.wv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data\n",
    "X_train = pd.DataFrame()\n",
    "y_train = train.set_index('node')\n",
    "\n",
    "for node in y_train.index:  # the sequence is not broken here\n",
    "    curr_sr = pd.Series(embeddings_model.wv[node], name=node)\n",
    "    curr_sr[len(curr_sr)] = degree_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = betweenness_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = closeness_centrality[node]\n",
    "    X_train = X_train.append(curr_sr)\n",
    "    \n",
    "X_train = torch.tensor(X_train.values)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "print(X_train.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data\n",
    "X_test = pd.DataFrame()\n",
    "y_test = test.set_index('node')\n",
    "\n",
    "for node in y_test.index:  # the sequence is not broken here\n",
    "    curr_sr = pd.Series(embeddings_model.wv[node], name=node)\n",
    "    curr_sr[len(curr_sr)] = degree_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = betweenness_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = closeness_centrality[node]\n",
    "    X_test = X_test.append(curr_sr)\n",
    "    \n",
    "X_test = torch.tensor(X_test.values)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "print(X_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 43])\n"
     ]
    }
   ],
   "source": [
    "print(X_test.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ffnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = FeedforwardNeuralNetModel(vector_size+3, 2*(vector_size+3), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(reg_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch num 0; loss is 0.7663506865501404;\n",
      "epoch num 100; loss is 0.6530712246894836;\n",
      "epoch num 200; loss is 0.5073510408401489;\n",
      "epoch num 300; loss is 0.38811585307121277;\n",
      "epoch num 400; loss is 0.3467429578304291;\n",
      "epoch num 500; loss is 0.46351158618927;\n",
      "epoch num 600; loss is 0.3678891360759735;\n",
      "epoch num 700; loss is 0.3344036042690277;\n",
      "epoch num 800; loss is 0.3755332827568054;\n",
      "epoch num 900; loss is 0.2690568268299103;\n"
     ]
    }
   ],
   "source": [
    "reg_model = FeedforwardNeuralNetModel(vector_size+3, 2 * (vector_size+3), 1)\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(reg_model.parameters(), lr=0.0001)\n",
    "\n",
    "batch_size = 2000\n",
    "\n",
    "for epoch in range(1000):  # it's usually tqdm here\n",
    "    order = np.random.permutation(len(X_train))\n",
    "    for start_index in range(0, len(X_train), batch_size):\n",
    "        batch_indexes = order[start_index:start_index+batch_size]\n",
    "        \n",
    "        X_batch = X_train[batch_indexes]\n",
    "        y_batch = y_train[batch_indexes]\n",
    "        \n",
    "        preds = reg_model.forward(X_batch) \n",
    "        \n",
    "        loss_value = loss(preds, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    reg_model.eval()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch num {epoch}; loss is {loss_value};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6125267"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, reg_model.forward(X_test).reshape(2000).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embeddings with Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "vector_size = 40  # just because it's about median for edges by each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# generator = Node2VecNodeGenerator(edges_sg, batch_size)\n",
    "# node_to_vec = Node2Vec(emb_size=vector_size, generator)\n",
    "# node_to_vec.in_out_tensors()\n",
    "\n",
    "# some mistake here, just on library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b6768f62b5451ea1f00c37e74865e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|████████████████| 20/20 [07:27<00:00, 22.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 51s, sys: 8.77 s, total: 22min\n",
      "Wall time: 21min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n2v = Node2Vec(edges_gnx, dimensions=vector_size, num_walks=20, walk_length=80, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 10s, sys: 35.8 s, total: 18min 46s\n",
      "Wall time: 16min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "node2vec_model = n2v.fit(window=20, min_count=5, batch_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node2vec_model.wv['11142'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data\n",
    "X_train = pd.DataFrame()\n",
    "y_train = train.set_index('node')\n",
    "\n",
    "for node in y_train.index:  # the sequence is not broken here\n",
    "    curr_sr = pd.Series(node2vec_model.wv[str(node)], name=node)\n",
    "    curr_sr[len(curr_sr)] = degree_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = betweenness_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = closeness_centrality[node]\n",
    "    X_train = X_train.append(curr_sr)\n",
    "    \n",
    "X_train = torch.tensor(X_train.values)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data\n",
    "X_test = pd.DataFrame()\n",
    "y_test = test.set_index('node')\n",
    "\n",
    "for node in y_test.index:  # the sequence is not broken here\n",
    "    curr_sr = pd.Series(node2vec_model.wv[str(node)], name=node)\n",
    "    curr_sr[len(curr_sr)] = degree_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = betweenness_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = closeness_centrality[node]\n",
    "    X_test = X_test.append(curr_sr)\n",
    "    \n",
    "X_test = torch.tensor(X_test.values)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "print(X_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch num 0; loss is 1.3015971183776855;\n",
      "epoch num 100; loss is 0.7300068140029907;\n",
      "epoch num 200; loss is 0.6180821657180786;\n",
      "epoch num 300; loss is 0.43670451641082764;\n",
      "epoch num 400; loss is 0.4730897545814514;\n",
      "epoch num 500; loss is 0.5012326836585999;\n",
      "epoch num 600; loss is 0.4272252917289734;\n",
      "epoch num 700; loss is 0.4739703834056854;\n",
      "epoch num 800; loss is 0.3758412301540375;\n",
      "epoch num 900; loss is 0.41991186141967773;\n"
     ]
    }
   ],
   "source": [
    "reg_model = FeedforwardNeuralNetModel(vector_size+3, 2 * (vector_size+3), 1)\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(reg_model.parameters(), lr=0.0001)\n",
    "\n",
    "batch_size = 2000\n",
    "\n",
    "for epoch in range(1000):  # it's usually tqdm here\n",
    "    order = np.random.permutation(len(X_train))\n",
    "    for start_index in range(0, len(X_train), batch_size):\n",
    "        batch_indexes = order[start_index:start_index+batch_size]\n",
    "        \n",
    "        X_batch = X_train[batch_indexes]\n",
    "        y_batch = y_train[batch_indexes]\n",
    "        \n",
    "        preds = reg_model.forward(X_batch) \n",
    "        \n",
    "        loss_value = loss(preds, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    reg_model.eval()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch num {epoch}; loss is {loss_value};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7861319"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, reg_model.forward(X_test).reshape(2000).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### result - lin regression of certainlities is the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next doesn't work here, because data don't have features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tried it, just for fun and skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame()\n",
    "y_train = train.set_index('node')\n",
    "\n",
    "X_test = pd.DataFrame()\n",
    "y_test = test.set_index('node')\n",
    "\n",
    "for node in y_train.index:  # the sequence is not broken here\n",
    "    curr_sr = pd.Series(embeddings_model.wv[node], name=node)\n",
    "    curr_sr[len(curr_sr)] = degree_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = betweenness_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = closeness_centrality[node]\n",
    "    X_train = X_train.append(curr_sr)\n",
    "    \n",
    "for node in y_test.index:\n",
    "    curr_sr = pd.Series(embeddings_model.wv[node], name=node)\n",
    "    curr_sr[len(curr_sr)] = degree_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = betweenness_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = closeness_centrality[node]\n",
    "    X_test = X_test.append(curr_sr)\n",
    "    \n",
    "X_all = pd.concat([X_train, X_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 43)\n"
     ]
    }
   ],
   "source": [
    "print(X_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.09 s, sys: 3.97 ms, total: 6.09 s\n",
      "Wall time: 6.09 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zendro/anaconda3/envs/for_mlops/lib/python3.8/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 64\n",
    "num_samples = [20, 20, 20]  # list of neighbours by depth\n",
    "layer_sizes = [vector_size + 3 , vector_size + 3, vector_size + 3]\n",
    "\n",
    "graph_nx = copy.deepcopy(edges_gnx)\n",
    "\n",
    "for node, row in X_all.iterrows():\n",
    "    graph_nx.nodes[node][\"feature\"] = row.to_numpy()\n",
    "\n",
    "edges_sg = StellarGraph.from_networkx(graph_nx, node_features='feature')\n",
    "\n",
    "unsupervised_samples = UnsupervisedSampler(\n",
    "    edges_sg,\n",
    "    nodes=list(edges_sg.nodes()),\n",
    "    length=80,\n",
    "    number_of_walks=20\n",
    ")\n",
    "\n",
    "generator = GraphSAGENodeGenerator(edges_sg, batch_size, num_samples=num_samples)\n",
    "node_gen = GraphSAGENodeGenerator(edges_sg, batch_size, num_samples).flow([node for node in list(edges_sg.nodes())])\n",
    "\n",
    "graphsage = GraphSAGE(layer_sizes=layer_sizes, generator=generator, bias=True)\n",
    "\n",
    "x_inp, x_out = graphsage.in_out_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = keras.Model(inputs=x_inp, outputs=x_out)\n",
    "embedding_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=keras.losses.MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 43)\n"
     ]
    }
   ],
   "source": [
    "node_embeddings = np.row_stack([embedding_model.predict(node[0], verbose=0) for node in node_gen])\n",
    "print(node_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "didn't get how i can use it, without features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame()\n",
    "y_train = train.set_index('node')\n",
    "\n",
    "X_test = pd.DataFrame()\n",
    "y_test = test.set_index('node')\n",
    "\n",
    "for node in y_train.index:  # the sequence is not broken here\n",
    "    curr_sr = pd.Series(embeddings_model.wv[node], name=node)\n",
    "    curr_sr[len(curr_sr)] = degree_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = betweenness_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = closeness_centrality[node]\n",
    "    X_train = X_train.append(curr_sr)\n",
    "    \n",
    "for node in y_test.index:\n",
    "    curr_sr = pd.Series(embeddings_model.wv[node], name=node)\n",
    "    curr_sr[len(curr_sr)] = degree_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = betweenness_centrality[node]\n",
    "    curr_sr[len(curr_sr)] = closeness_centrality[node]\n",
    "    X_test = X_test.append(curr_sr)\n",
    "    \n",
    "X_all = pd.concat([X_train, X_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GCN (local pooling) filters...\n"
     ]
    }
   ],
   "source": [
    "graph_nx = copy.deepcopy(edges_gnx)\n",
    "\n",
    "for node, row in X_all.iterrows():\n",
    "    graph_nx.nodes[node][\"feature\"] = row.to_numpy()\n",
    "    \n",
    "edges_sg = StellarGraph.from_networkx(graph_nx, node_features='feature')\n",
    "\n",
    "generator = FullBatchNodeGenerator(edges_sg, method=\"gcn\")\n",
    "gcn = GCN(\n",
    "    layer_sizes=[16, 16], activations=[\"relu\", \"relu\"],\n",
    "    generator=generator, dropout=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
